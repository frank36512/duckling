"""
æœºå™¨å­¦ä¹ æ¨¡å‹è®­ç»ƒå¼•æ“
æ”¯æŒRandomForestã€LSTMã€XGBoostç­–ç•¥çš„æ¨¡å‹è®­ç»ƒ
"""

import logging
import numpy as np
import pandas as pd
from typing import Dict, Any, Optional, Tuple, Callable
from datetime import datetime
import os
import pickle
import json

logger = logging.getLogger(__name__)


class MLTrainer:
    """æœºå™¨å­¦ä¹ è®­ç»ƒå™¨åŸºç±»"""
    
    def __init__(self, strategy_name: str, config: Dict[str, Any] = None):
        """
        åˆå§‹åŒ–è®­ç»ƒå™¨
        
        :param strategy_name: ç­–ç•¥åç§° (RandomForest/LSTM/XGBoost)
        :param config: é…ç½®å‚æ•°
        """
        self.strategy_name = strategy_name
        self.config = config or {}
        self.model = None
        self.scaler = None
        self.feature_names = []
        
        # åˆ›å»ºæ¨¡å‹ä¿å­˜ç›®å½•
        self.model_dir = os.path.join(os.path.dirname(os.path.dirname(__file__)), 'models')
        os.makedirs(self.model_dir, exist_ok=True)
        
        logger.info(f"åˆå§‹åŒ– {strategy_name} è®­ç»ƒå™¨")
    
    def prepare_features(self, data: pd.DataFrame) -> pd.DataFrame:
        """
        å‡†å¤‡ç‰¹å¾æ•°æ®
        
        :param data: åŸå§‹è‚¡ç¥¨æ•°æ® (åŒ…å« open, high, low, close, volume)
        :return: ç‰¹å¾æ•°æ®æ¡†
        """
        logger.info("å¼€å§‹ç‰¹å¾å·¥ç¨‹...")
        df = data.copy()
        
        # 1. ä»·æ ¼ç‰¹å¾
        # ç§»åŠ¨å¹³å‡çº¿
        df['sma_5'] = df['close'].rolling(window=5).mean()
        df['sma_10'] = df['close'].rolling(window=10).mean()
        df['sma_20'] = df['close'].rolling(window=20).mean()
        df['sma_60'] = df['close'].rolling(window=60).mean()
        
        # æŒ‡æ•°ç§»åŠ¨å¹³å‡
        df['ema_12'] = df['close'].ewm(span=12).mean()
        df['ema_26'] = df['close'].ewm(span=26).mean()
        
        # 2. åŠ¨é‡æŒ‡æ ‡
        # RSI
        delta = df['close'].diff()
        gain = (delta.where(delta > 0, 0)).rolling(window=14).mean()
        loss = (-delta.where(delta < 0, 0)).rolling(window=14).mean()
        rs = gain / loss
        df['rsi'] = 100 - (100 / (1 + rs))
        
        # MACD
        df['macd'] = df['ema_12'] - df['ema_26']
        df['macd_signal'] = df['macd'].ewm(span=9).mean()
        df['macd_hist'] = df['macd'] - df['macd_signal']
        
        # ROC (Rate of Change)
        df['roc'] = df['close'].pct_change(periods=10) * 100
        
        # 3. æ³¢åŠ¨ç‡æŒ‡æ ‡
        # ATR (Average True Range)
        high_low = df['high'] - df['low']
        high_close = np.abs(df['high'] - df['close'].shift())
        low_close = np.abs(df['low'] - df['close'].shift())
        tr = pd.concat([high_low, high_close, low_close], axis=1).max(axis=1)
        df['atr'] = tr.rolling(window=14).mean()
        
        # å¸ƒæ—å¸¦
        df['bb_middle'] = df['close'].rolling(window=20).mean()
        bb_std = df['close'].rolling(window=20).std()
        df['bb_upper'] = df['bb_middle'] + (bb_std * 2)
        df['bb_lower'] = df['bb_middle'] - (bb_std * 2)
        df['bb_width'] = (df['bb_upper'] - df['bb_lower']) / df['bb_middle']
        
        # 4. æˆäº¤é‡æŒ‡æ ‡
        df['volume_sma'] = df['volume'].rolling(window=20).mean()
        df['volume_ratio'] = df['volume'] / df['volume_sma']
        
        # OBV (On Balance Volume)
        df['obv'] = (np.sign(df['close'].diff()) * df['volume']).fillna(0).cumsum()
        
        # 5. ä»·æ ¼å˜åŒ–ç‰¹å¾
        df['price_change'] = df['close'].pct_change()
        df['price_change_5'] = df['close'].pct_change(periods=5)
        df['price_change_10'] = df['close'].pct_change(periods=10)
        
        # 6. é«˜ä½ç‚¹ç‰¹å¾
        df['high_low_ratio'] = df['high'] / df['low']
        df['close_open_ratio'] = df['close'] / df['open']
        
        # 7. è¶‹åŠ¿ç‰¹å¾
        df['trend_5'] = (df['close'] > df['sma_5']).astype(int)
        df['trend_20'] = (df['close'] > df['sma_20']).astype(int)
        
        # åˆ é™¤NaNå€¼
        df = df.dropna()
        
        logger.info(f"ç‰¹å¾å·¥ç¨‹å®Œæˆï¼Œç”Ÿæˆ {len(df.columns)} ä¸ªç‰¹å¾ï¼Œ{len(df)} æ¡æ ·æœ¬")
        
        return df
    
    def create_labels(self, data: pd.DataFrame, horizon: int = 1, threshold: float = 0.02) -> pd.Series:
        """
        åˆ›å»ºè®­ç»ƒæ ‡ç­¾
        
        :param data: ç‰¹å¾æ•°æ®
        :param horizon: é¢„æµ‹æ—¶é—´èŒƒå›´ï¼ˆå¤©ï¼‰
        :param threshold: æ¶¨è·Œé˜ˆå€¼
        :return: æ ‡ç­¾åºåˆ— (1: ä¸Šæ¶¨, 0: ä¸‹è·Œ)
        """
        # è®¡ç®—æœªæ¥æ”¶ç›Š
        future_return = data['close'].shift(-horizon) / data['close'] - 1
        
        # åˆ›å»ºäºŒåˆ†ç±»æ ‡ç­¾
        labels = (future_return > threshold).astype(int)
        
        logger.info(f"æ ‡ç­¾åˆ†å¸ƒ - ä¸Šæ¶¨: {labels.sum()}, ä¸‹è·Œ: {len(labels) - labels.sum()}")
        
        return labels
    
    def split_data(
        self, 
        features: pd.DataFrame, 
        labels: pd.Series,
        train_ratio: float = 0.8,
        val_ratio: float = 0.1
    ) -> Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame, pd.Series, pd.Series, pd.Series]:
        """
        åˆ†å‰²è®­ç»ƒé›†ã€éªŒè¯é›†ã€æµ‹è¯•é›†
        
        :param features: ç‰¹å¾æ•°æ®
        :param labels: æ ‡ç­¾æ•°æ®
        :param train_ratio: è®­ç»ƒé›†æ¯”ä¾‹
        :param val_ratio: éªŒè¯é›†æ¯”ä¾‹
        :return: (X_train, X_val, X_test, y_train, y_val, y_test)
        """
        n = len(features)
        train_size = int(n * train_ratio)
        val_size = int(n * val_ratio)
        
        # æ—¶é—´åºåˆ—æ•°æ®ï¼ŒæŒ‰é¡ºåºåˆ†å‰²
        X_train = features.iloc[:train_size]
        y_train = labels.iloc[:train_size]
        
        X_val = features.iloc[train_size:train_size + val_size]
        y_val = labels.iloc[train_size:train_size + val_size]
        
        X_test = features.iloc[train_size + val_size:]
        y_test = labels.iloc[train_size + val_size:]
        
        logger.info(f"æ•°æ®åˆ†å‰² - è®­ç»ƒé›†: {len(X_train)}, éªŒè¯é›†: {len(X_val)}, æµ‹è¯•é›†: {len(X_test)}")
        
        return X_train, X_val, X_test, y_train, y_val, y_test
    
    def normalize_features(self, X_train: pd.DataFrame, X_val: pd.DataFrame, X_test: pd.DataFrame):
        """
        ç‰¹å¾æ ‡å‡†åŒ–
        
        :param X_train: è®­ç»ƒé›†ç‰¹å¾
        :param X_val: éªŒè¯é›†ç‰¹å¾
        :param X_test: æµ‹è¯•é›†ç‰¹å¾
        :return: æ ‡å‡†åŒ–åçš„æ•°æ®
        """
        from sklearn.preprocessing import StandardScaler
        
        self.scaler = StandardScaler()
        
        # åªåœ¨è®­ç»ƒé›†ä¸Šfit
        X_train_scaled = self.scaler.fit_transform(X_train)
        X_val_scaled = self.scaler.transform(X_val)
        X_test_scaled = self.scaler.transform(X_test)
        
        logger.info("ç‰¹å¾æ ‡å‡†åŒ–å®Œæˆ")
        
        return X_train_scaled, X_val_scaled, X_test_scaled
    
    def evaluate_model(self, y_true, y_pred, set_name: str = "æµ‹è¯•é›†") -> Dict[str, float]:
        """
        è¯„ä¼°æ¨¡å‹æ€§èƒ½
        
        :param y_true: çœŸå®æ ‡ç­¾
        :param y_pred: é¢„æµ‹æ ‡ç­¾
        :param set_name: æ•°æ®é›†åç§°
        :return: è¯„ä¼°æŒ‡æ ‡å­—å…¸
        """
        from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix
        
        metrics = {
            'accuracy': accuracy_score(y_true, y_pred),
            'precision': precision_score(y_true, y_pred, zero_division=0),
            'recall': recall_score(y_true, y_pred, zero_division=0),
            'f1': f1_score(y_true, y_pred, zero_division=0)
        }
        
        cm = confusion_matrix(y_true, y_pred)
        
        logger.info(f"\n{'='*60}")
        logger.info(f"{set_name}è¯„ä¼°ç»“æœ:")
        logger.info(f"  å‡†ç¡®ç‡ (Accuracy):  {metrics['accuracy']:.4f}")
        logger.info(f"  ç²¾ç¡®ç‡ (Precision): {metrics['precision']:.4f}")
        logger.info(f"  å¬å›ç‡ (Recall):    {metrics['recall']:.4f}")
        logger.info(f"  F1åˆ†æ•° (F1-Score):  {metrics['f1']:.4f}")
        logger.info(f"\næ··æ·†çŸ©é˜µ:")
        logger.info(f"  TN: {cm[0,0]:4d}  FP: {cm[0,1]:4d}")
        logger.info(f"  FN: {cm[1,0]:4d}  TP: {cm[1,1]:4d}")
        logger.info(f"{'='*60}")
        
        return metrics
    
    def save_model(self, stock_code: str, metadata: Dict[str, Any] = None):
        """
        ä¿å­˜æ¨¡å‹å’Œé…ç½®
        
        :param stock_code: è‚¡ç¥¨ä»£ç 
        :param metadata: å…ƒæ•°æ®
        """
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        model_name = f"{self.strategy_name}_{stock_code}_{timestamp}"
        
        # ä¿å­˜æ¨¡å‹
        model_path = os.path.join(self.model_dir, f"{model_name}.pkl")
        with open(model_path, 'wb') as f:
            pickle.dump({
                'model': self.model,
                'scaler': self.scaler,
                'feature_names': self.feature_names,
                'strategy_name': self.strategy_name,
                'stock_code': stock_code,
                'timestamp': timestamp,
                'metadata': metadata or {}
            }, f)
        
        logger.info(f"æ¨¡å‹å·²ä¿å­˜: {model_path}")
        
        # ä¿å­˜é…ç½®
        config_path = os.path.join(self.model_dir, f"{model_name}.json")
        with open(config_path, 'w', encoding='utf-8') as f:
            json.dump({
                'strategy_name': self.strategy_name,
                'stock_code': stock_code,
                'timestamp': timestamp,
                'feature_count': len(self.feature_names),
                'metadata': metadata or {}
            }, f, indent=2, ensure_ascii=False)
        
        logger.info(f"é…ç½®å·²ä¿å­˜: {config_path}")
        
        return model_path
    
    def train(
        self, 
        data: pd.DataFrame,
        progress_callback: Optional[Callable[[str], None]] = None
    ) -> Dict[str, Any]:
        """
        è®­ç»ƒæ¨¡å‹ï¼ˆå­ç±»å®ç°ï¼‰
        
        :param data: è®­ç»ƒæ•°æ®
        :param progress_callback: è¿›åº¦å›è°ƒå‡½æ•°
        :return: è®­ç»ƒç»“æœ
        """
        raise NotImplementedError("å­ç±»éœ€è¦å®ç° train æ–¹æ³•")


class RandomForestTrainer(MLTrainer):
    """éšæœºæ£®æ—è®­ç»ƒå™¨"""
    
    def __init__(self, config: Dict[str, Any] = None):
        super().__init__('RandomForest', config)
    
    def train(
        self, 
        data: pd.DataFrame,
        progress_callback: Optional[Callable[[str], None]] = None
    ) -> Dict[str, Any]:
        """
        è®­ç»ƒéšæœºæ£®æ—æ¨¡å‹
        """
        from sklearn.ensemble import RandomForestClassifier
        
        def log(msg):
            logger.info(msg)
            if progress_callback:
                progress_callback(msg)
        
        log("ğŸŒ² å¼€å§‹è®­ç»ƒéšæœºæ£®æ—æ¨¡å‹...")
        
        # 1. ç‰¹å¾å·¥ç¨‹
        log("ğŸ“Š æ­¥éª¤ 1/6: ç‰¹å¾å·¥ç¨‹...")
        features_df = self.prepare_features(data)
        
        # 2. åˆ›å»ºæ ‡ç­¾
        log("ğŸ·ï¸  æ­¥éª¤ 2/6: åˆ›å»ºæ ‡ç­¾...")
        labels = self.create_labels(features_df, horizon=1, threshold=0.02)
        
        # å¯¹é½ç‰¹å¾å’Œæ ‡ç­¾
        features_df = features_df[:-1]  # ç§»é™¤æœ€åä¸€è¡Œï¼ˆæ²¡æœ‰æ ‡ç­¾ï¼‰
        labels = labels[:-1]
        
        # é€‰æ‹©ç‰¹å¾åˆ—
        feature_cols = [col for col in features_df.columns if col not in ['open', 'high', 'low', 'close', 'volume']]
        X = features_df[feature_cols]
        y = labels
        
        self.feature_names = feature_cols
        
        # 3. åˆ†å‰²æ•°æ®
        log("âœ‚ï¸  æ­¥éª¤ 3/6: åˆ†å‰²æ•°æ®é›†...")
        train_ratio = self.config.get('train_ratio', 0.8)
        val_ratio = self.config.get('val_ratio', 0.1)
        X_train, X_val, X_test, y_train, y_val, y_test = self.split_data(X, y, train_ratio, val_ratio)
        
        # 4. ç‰¹å¾æ ‡å‡†åŒ–
        log("ğŸ”„ æ­¥éª¤ 4/6: ç‰¹å¾æ ‡å‡†åŒ–...")
        X_train_scaled, X_val_scaled, X_test_scaled = self.normalize_features(X_train, X_val, X_test)
        
        # 5. è®­ç»ƒæ¨¡å‹
        log("ğŸš€ æ­¥éª¤ 5/6: è®­ç»ƒéšæœºæ£®æ—...")
        n_estimators = self.config.get('n_estimators', 100)
        max_depth = self.config.get('max_depth', 10)
        
        self.model = RandomForestClassifier(
            n_estimators=n_estimators,
            max_depth=max_depth,
            random_state=42,
            n_jobs=-1,
            verbose=1
        )
        
        self.model.fit(X_train_scaled, y_train)
        log(f"âœ… æ¨¡å‹è®­ç»ƒå®Œæˆï¼ä½¿ç”¨ {n_estimators} æ£µæ ‘ï¼Œæœ€å¤§æ·±åº¦ {max_depth}")
        
        # 6. è¯„ä¼°æ¨¡å‹
        log("ğŸ“ˆ æ­¥éª¤ 6/6: è¯„ä¼°æ¨¡å‹æ€§èƒ½...")
        
        y_train_pred = self.model.predict(X_train_scaled)
        train_metrics = self.evaluate_model(y_train, y_train_pred, "è®­ç»ƒé›†")
        
        y_val_pred = self.model.predict(X_val_scaled)
        val_metrics = self.evaluate_model(y_val, y_val_pred, "éªŒè¯é›†")
        
        y_test_pred = self.model.predict(X_test_scaled)
        test_metrics = self.evaluate_model(y_test, y_test_pred, "æµ‹è¯•é›†")
        
        # ç‰¹å¾é‡è¦æ€§
        feature_importance = pd.DataFrame({
            'feature': self.feature_names,
            'importance': self.model.feature_importances_
        }).sort_values('importance', ascending=False)
        
        log("\nğŸ“Š Top 10 é‡è¦ç‰¹å¾:")
        for idx, row in feature_importance.head(10).iterrows():
            log(f"  {row['feature']:20s}: {row['importance']:.4f}")
        
        results = {
            'train_metrics': train_metrics,
            'val_metrics': val_metrics,
            'test_metrics': test_metrics,
            'feature_importance': feature_importance.to_dict('records'),
            'n_samples': len(data),
            'n_features': len(self.feature_names)
        }
        
        log("\nğŸ‰ éšæœºæ£®æ—æ¨¡å‹è®­ç»ƒå®Œæˆï¼")
        
        return results


class XGBoostTrainer(MLTrainer):
    """XGBoostè®­ç»ƒå™¨"""
    
    def __init__(self, config: Dict[str, Any] = None):
        super().__init__('XGBoost', config)
    
    def train(
        self, 
        data: pd.DataFrame,
        progress_callback: Optional[Callable[[str], None]] = None
    ) -> Dict[str, Any]:
        """
        è®­ç»ƒXGBoostæ¨¡å‹
        """
        try:
            import xgboost as xgb
        except ImportError:
            logger.error("XGBoostæœªå®‰è£…ï¼Œè¯·è¿è¡Œ: pip install xgboost")
            raise ImportError("éœ€è¦å®‰è£… xgboost åº“")
        
        def log(msg):
            logger.info(msg)
            if progress_callback:
                progress_callback(msg)
        
        log("âš¡ å¼€å§‹è®­ç»ƒXGBoostæ¨¡å‹...")
        
        # 1. ç‰¹å¾å·¥ç¨‹
        log("ğŸ“Š æ­¥éª¤ 1/6: ç‰¹å¾å·¥ç¨‹...")
        features_df = self.prepare_features(data)
        
        # 2. åˆ›å»ºæ ‡ç­¾
        log("ğŸ·ï¸  æ­¥éª¤ 2/6: åˆ›å»ºæ ‡ç­¾...")
        labels = self.create_labels(features_df, horizon=1, threshold=0.02)
        
        # å¯¹é½
        features_df = features_df[:-1]
        labels = labels[:-1]
        
        # é€‰æ‹©ç‰¹å¾
        feature_cols = [col for col in features_df.columns if col not in ['open', 'high', 'low', 'close', 'volume']]
        X = features_df[feature_cols]
        y = labels
        
        self.feature_names = feature_cols
        
        # 3. åˆ†å‰²æ•°æ®
        log("âœ‚ï¸  æ­¥éª¤ 3/6: åˆ†å‰²æ•°æ®é›†...")
        train_ratio = self.config.get('train_ratio', 0.8)
        val_ratio = self.config.get('val_ratio', 0.1)
        X_train, X_val, X_test, y_train, y_val, y_test = self.split_data(X, y, train_ratio, val_ratio)
        
        # 4. ç‰¹å¾æ ‡å‡†åŒ–
        log("ğŸ”„ æ­¥éª¤ 4/6: ç‰¹å¾æ ‡å‡†åŒ–...")
        X_train_scaled, X_val_scaled, X_test_scaled = self.normalize_features(X_train, X_val, X_test)
        
        # 5. è®­ç»ƒæ¨¡å‹
        log("ğŸš€ æ­¥éª¤ 5/6: è®­ç»ƒXGBoost...")
        n_estimators = self.config.get('n_estimators', 100)
        max_depth = self.config.get('max_depth', 5)
        learning_rate = self.config.get('learning_rate', 0.1)
        
        self.model = xgb.XGBClassifier(
            n_estimators=n_estimators,
            max_depth=max_depth,
            learning_rate=learning_rate,
            random_state=42,
            n_jobs=-1,
            eval_metric='logloss'
        )
        
        self.model.fit(
            X_train_scaled, y_train,
            eval_set=[(X_val_scaled, y_val)],
            verbose=True
        )
        log(f"âœ… æ¨¡å‹è®­ç»ƒå®Œæˆï¼")
        
        # 6. è¯„ä¼°
        log("ğŸ“ˆ æ­¥éª¤ 6/6: è¯„ä¼°æ¨¡å‹æ€§èƒ½...")
        
        y_train_pred = self.model.predict(X_train_scaled)
        train_metrics = self.evaluate_model(y_train, y_train_pred, "è®­ç»ƒé›†")
        
        y_val_pred = self.model.predict(X_val_scaled)
        val_metrics = self.evaluate_model(y_val, y_val_pred, "éªŒè¯é›†")
        
        y_test_pred = self.model.predict(X_test_scaled)
        test_metrics = self.evaluate_model(y_test, y_test_pred, "æµ‹è¯•é›†")
        
        # ç‰¹å¾é‡è¦æ€§
        feature_importance = pd.DataFrame({
            'feature': self.feature_names,
            'importance': self.model.feature_importances_
        }).sort_values('importance', ascending=False)
        
        log("\nğŸ“Š Top 10 é‡è¦ç‰¹å¾:")
        for idx, row in feature_importance.head(10).iterrows():
            log(f"  {row['feature']:20s}: {row['importance']:.4f}")
        
        results = {
            'train_metrics': train_metrics,
            'val_metrics': val_metrics,
            'test_metrics': test_metrics,
            'feature_importance': feature_importance.to_dict('records'),
            'n_samples': len(data),
            'n_features': len(self.feature_names)
        }
        
        log("\nğŸ‰ XGBoostæ¨¡å‹è®­ç»ƒå®Œæˆï¼")
        
        return results


def create_trainer(strategy_name: str, config: Dict[str, Any] = None) -> MLTrainer:
    """
    åˆ›å»ºè®­ç»ƒå™¨å·¥å‚å‡½æ•°
    
    :param strategy_name: ç­–ç•¥åç§°
    :param config: é…ç½®å‚æ•°
    :return: è®­ç»ƒå™¨å®ä¾‹
    """
    if strategy_name == 'RandomForest':
        return RandomForestTrainer(config)
    elif strategy_name == 'XGBoost':
        return XGBoostTrainer(config)
    elif strategy_name == 'LSTM':
        # LSTMè®­ç»ƒå™¨æš‚æœªå®ç°
        raise NotImplementedError("LSTMè®­ç»ƒå™¨å¼€å‘ä¸­ï¼Œè¯·ä½¿ç”¨RandomForestæˆ–XGBoost")
    else:
        raise ValueError(f"ä¸æ”¯æŒçš„ç­–ç•¥: {strategy_name}")
